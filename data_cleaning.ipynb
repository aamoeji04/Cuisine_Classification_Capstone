{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:16:11.636591Z",
     "start_time": "2021-02-12T09:16:11.606172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer #\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords #\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# make sure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:15:12.678481Z",
     "start_time": "2021-02-12T09:15:12.600329Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 ½kg chicken thighs and drumsticks 180g sea s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 large chicken 5 beef short ribs (about 3kg),...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150g unsalted butter , softened 80g golden cas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 whole chicken , jointed, or 8 bone-in chicke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500g macaroni 1l whole milk 2 bay leaves 60g b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Ingredients  Cuisine\n",
       "0  1 ½kg chicken thighs and drumsticks 180g sea s...        1\n",
       "1  1 large chicken 5 beef short ribs (about 3kg),...        1\n",
       "2  150g unsalted butter , softened 80g golden cas...        1\n",
       "3  1 whole chicken , jointed, or 8 bone-in chicke...        1\n",
       "4  500g macaroni 1l whole milk 2 bay leaves 60g b...        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final_bbc_data.csv', index_col = 0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:41:52.794475Z",
     "start_time": "2021-02-11T20:41:52.774530Z"
    }
   },
   "outputs": [],
   "source": [
    "# grab specified string from dataframe\n",
    "def testing_str(index_num):\n",
    "    read = df['Ingredients'][index_num]\n",
    "    return read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:32:18.148745Z",
     "start_time": "2021-02-12T09:32:18.129443Z"
    }
   },
   "outputs": [],
   "source": [
    "def shallow_cleaning(_corpus):\n",
    "    _corpus = _corpus.lower() # lowercase\n",
    "    _corpus = re.sub('\\[.*/()]', '', _corpus) # removes data in brackets\n",
    "    _corpus = re.sub('[%s]' % re.escape(string.punctuation), '', _corpus) # list of punctuation, get rid of any punctuation\n",
    "    _corpus = re.sub('\\w*\\d\\w*', '', _corpus) # removes all numbers and any words that comtain them\n",
    "    return _corpus\n",
    "# apply the function and assign to variable\n",
    "shallow_clean = lambda x: shallow_cleaning(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:32:18.454231Z",
     "start_time": "2021-02-12T09:32:18.382304Z"
    }
   },
   "outputs": [],
   "source": [
    "shallow_ingredients = pd.DataFrame(df['Ingredients'].apply(shallow_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:32:18.860979Z",
     "start_time": "2021-02-12T09:32:18.833380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>½kg chicken thighs and drumsticks  sea salt f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>large chicken  beef short ribs about  ribs se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unsalted butter  softened  golden caster suga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole chicken  jointed or  bonein chicken pie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macaroni  whole milk  bay leaves  butter  pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>urid dal black gram  vegetable oil  tsp fresh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>strong white flour   tsp salt  sachet fastact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>chopped rhubarb  light soft brown sugar  tsp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>gram flour  selfraising flour ½ tsp red chill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>tbsp vegetable oil  piece ginger grated  larg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Ingredients\n",
       "0     ½kg chicken thighs and drumsticks  sea salt f...\n",
       "1     large chicken  beef short ribs about  ribs se...\n",
       "2     unsalted butter  softened  golden caster suga...\n",
       "3     whole chicken  jointed or  bonein chicken pie...\n",
       "4     macaroni  whole milk  bay leaves  butter  pla...\n",
       "..                                                 ...\n",
       "895   urid dal black gram  vegetable oil  tsp fresh...\n",
       "896   strong white flour   tsp salt  sachet fastact...\n",
       "897    chopped rhubarb  light soft brown sugar  tsp...\n",
       "898   gram flour  selfraising flour ½ tsp red chill...\n",
       "899   tbsp vegetable oil  piece ginger grated  larg...\n",
       "\n",
       "[900 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shallow_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:28:00.304710Z",
     "start_time": "2021-02-12T09:28:00.296705Z"
    }
   },
   "outputs": [],
   "source": [
    "shallow_df = shallow_ingredients.join(df['Cuisine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fdist(arg, n):\n",
    "    fdist = FreqDist(arg)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    return fdist.plot(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:30:10.851622Z",
     "start_time": "2021-02-11T20:30:10.836278Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'kg',\n",
       " 'chicken',\n",
       " 'thighs',\n",
       " 'and',\n",
       " 'drumsticks',\n",
       " '180g',\n",
       " 'sea',\n",
       " 'salt',\n",
       " 'flakes',\n",
       " '90g',\n",
       " 'soft',\n",
       " 'dark',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " '2',\n",
       " 'tbsp',\n",
       " 'chilli',\n",
       " 'flakes',\n",
       " '2',\n",
       " 'tbsp',\n",
       " 'sweet',\n",
       " 'smoked',\n",
       " 'paprika',\n",
       " '1',\n",
       " 'tbsp',\n",
       " 'ground',\n",
       " 'cumin',\n",
       " '1',\n",
       " 'tbsp',\n",
       " 'sea',\n",
       " 'salt',\n",
       " 'flakes',\n",
       " '1',\n",
       " 'tbsp',\n",
       " 'dark',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " '75g',\n",
       " 'butter',\n",
       " '125ml',\n",
       " 'hot',\n",
       " 'chilli',\n",
       " 'sauce',\n",
       " '1',\n",
       " 'tbsp',\n",
       " 'maple',\n",
       " 'syrup']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split documents into tokens (xgrams, stopwords, etc.)\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tokenized_read = tokenizer.tokenize(read)\n",
    "# words have been turned into individual tokens\n",
    "tokenized_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will probably need custom stop words for the culinary dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T07:21:21.563447Z",
     "start_time": "2021-02-12T07:21:21.555070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# compare the lengths of filtered and unfiltered\n",
    "print(len(tokenized_read))\n",
    "print(len(filtered_read))\n",
    "# 'and' is the filtered out word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:07:15.868647Z",
     "start_time": "2021-02-11T20:07:15.860579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'kg', 'chicken', 'thigh', 'drumstick', '180g', 'sea', 'salt', 'flake', '90g', 'soft', 'dark', 'brown', 'sugar', '2', 'tbsp', 'chilli', 'flake', '2', 'tbsp', 'sweet', 'smoke', 'paprika', '1', 'tbsp', 'ground', 'cumin', '1', 'tbsp', 'sea', 'salt', 'flake', '1', 'tbsp', 'dark', 'brown', 'sugar', '75g', 'butter', '125ml', 'hot', 'chilli', 'sauc', '1', 'tbsp', 'mapl', 'syrup']\n"
     ]
    }
   ],
   "source": [
    "# Stemming: change = chang\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_read=[]\n",
    "for w in filtered_read:\n",
    "    stemmed_read.append(ps.stem(w))\n",
    "\n",
    "print(stemmed_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:09:30.557730Z",
     "start_time": "2021-02-11T20:09:29.063416Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'kg', 'chicken', 'thigh', 'drumstick', '180g', 'sea', 'salt', 'flake', '90g', 'soft', 'dark', 'brown', 'sugar', '2', 'tbsp', 'chilli', 'flake', '2', 'tbsp', 'sweet', 'smoked', 'paprika', '1', 'tbsp', 'ground', 'cumin', '1', 'tbsp', 'sea', 'salt', 'flake', '1', 'tbsp', 'dark', 'brown', 'sugar', '75g', 'butter', '125ml', 'hot', 'chilli', 'sauce', '1', 'tbsp', 'maple', 'syrup']\n"
     ]
    }
   ],
   "source": [
    "# lemmatezization: changer = change\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "lemmatized_read=[]\n",
    "for w in filtered_read:\n",
    "    lemmatized_read.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "print(lemmatized_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:35:24.052243Z",
     "start_time": "2021-02-11T20:35:24.044204Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_read = tokenizer.tokenize(read)\n",
    "tokenized_read_2 = tokenizer.tokenize(read_2)\n",
    "tokenized_read_3 = tokenizer.tokenize(read_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:35:30.775189Z",
     "start_time": "2021-02-11T20:35:30.766910Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dried',\n",
       " 'morita',\n",
       " 'chillies',\n",
       " 'dried',\n",
       " 'ancho',\n",
       " 'chilli',\n",
       " 'dried',\n",
       " 'pasilla',\n",
       " 'mixe',\n",
       " 'chilli',\n",
       " 'garlic',\n",
       " 'cloves',\n",
       " 'sea',\n",
       " 'salt',\n",
       " 'tbsp',\n",
       " 'balsamic',\n",
       " 'vinegar',\n",
       " 'avocado',\n",
       " 'leaves',\n",
       " 'grapeseed',\n",
       " 'oil',\n",
       " 'plus',\n",
       " 'more',\n",
       " 'for',\n",
       " 'frying',\n",
       " 'lamb',\n",
       " 'shanks',\n",
       " 'banana',\n",
       " 'leaves',\n",
       " 'vegetable',\n",
       " 'or',\n",
       " 'beef',\n",
       " 'stock',\n",
       " 'bulb',\n",
       " 'garlic',\n",
       " 'cloves',\n",
       " 'peeled',\n",
       " 'red',\n",
       " 'onions',\n",
       " 'sliced',\n",
       " 'tomatoes',\n",
       " 'thickly',\n",
       " 'sliced',\n",
       " 'tomatillos',\n",
       " 'roughly',\n",
       " 'chopped',\n",
       " 'green',\n",
       " 'jalape',\n",
       " 'os',\n",
       " 'garlic',\n",
       " 'cloves',\n",
       " 'peeled',\n",
       " 'small',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'coriander',\n",
       " 'lime',\n",
       " 'juiced',\n",
       " 'avocado',\n",
       " 'peeled',\n",
       " 'with',\n",
       " 'stone',\n",
       " 'removed',\n",
       " 'corn',\n",
       " 'tortillas',\n",
       " 'Mexican',\n",
       " 'crema',\n",
       " 'or',\n",
       " 'soured',\n",
       " 'cream',\n",
       " 'queso',\n",
       " 'fresco',\n",
       " 'or',\n",
       " 'feta',\n",
       " 'cos',\n",
       " 'lettuce',\n",
       " 'shredded',\n",
       " 'toothpicks']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_read_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing it in python\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# creates bagofwords\n",
    "# turns documents into bag of words\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "docs = ['i love dogs','i love cats','i love all animals']\n",
    "\n",
    "# you dont have to tokenize the words\n",
    "# jujst create the countvecotizer and pass in parameters\n",
    "vec = CountVectorizer(stop_words=stop_words, lowercase = True, ngram_range = (1,2)) #give me unigram and end with bigram\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
